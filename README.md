# Awesome AI Agent Leaderboards

<p align="center">
  <img src="https://github.com/user-attachments/assets/2fc0cc17-4625-4368-afba-f373b442488c" alt="Supernal Intelligence Logo" width="700"/>
</p>


<p align="center">
  <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT"></a>
  <a href="https://github.com/orgs/supernalintelligence/repositories"><img src="https://img.shields.io/github/stars/supernalai/awesome-ai-agent-benchmarks?style=social" alt="Stars"></a>
</p>

<p align="center">
  <a href="https://x.com/supernalasi"><img src="https://img.shields.io/badge/follow-%40supernalasi-1DA1F2?logo=x&style=social" alt="Follow on X"></a>
  <a href="https://bsky.app/profile/supernalasi.bsky.social"><img src="https://img.shields.io/badge/Bluesky-supernalasi.bsky.social-3A88FE?style=flat&logo=bluesky&logoColor=white" alt="Bluesky"></a>
  <a href="https://supernalintelligence.com"><img src="https://img.shields.io/badge/Website-supernalintelligence.com-blue?style=flat&logo=safari&logoColor=white" alt="Website"></a>
</p>



A curated list of leaderboards for AI agents and frontier models. This resource tracks the capabilities of AI agent, providing insights into progress toward artificial super intelligence (ASI).

This project is maintained by [Parni](https://x.com/ParnianBrk) and [Ian](https://x.com/ian_derrington). Follow [Supernal Intelligence](https://x.com/supernalasi) for more updates.

**Website**: [supernalintelligence.com](https://www.supernalintelligence.com/)  
**Join our Discord**: [Supernal Intelligence Discord](https://discord.gg/J9pU82wP)

For more complete data and the latest information, please visit our website: [supernalintelligence.com](https://www.supernalintelligence.com/)

If you see an error or want to contribute, please email [i@supernal.ai](mailto:i@supernal.ai)

## Meta-Benchmarks

### Evaluation Platforms

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| GAIA Leaderboard | [gaia-benchmark.github.io](https://gaia-benchmark.github.io/) | General AI Assistants benchmark focused on tool use, web browsing, reasoning, and multimodal tasks | HAL Team at Princeton | HAL Team Models |
| Scale AI SEAL | [scale.com/seal](https://scale.com/seal) | Frontier reasoning benchmark focused on honesty, safety, and complex reasoning | Scale AI | Proprietary Models |
| LLM-Stats.com | [llm-stats.com](https://llm-stats.com/) | LLM Performance Metrics focusing on context length, latency, and cost | Independent | Claude 3.5, GPT-4o |
| LMSYS Chatbot Arena | [lmsys.org/chatbot_arena](https://lmsys.org/chatbot_arena/) | Human preference rankings for conversational AI with an open leaderboard | LMSYS Org | Claude 3 Opus |
| MT-Bench | [github.com/lm-sys/FastChat](https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge) | Multi-turn conversation benchmark | LMSYS Org | Claude 3 Opus |
| Agent Leaderboard | / | Evaluating autonomous agent capabilities | Various | N/A |
| HELM | [crfm.stanford.edu/helm](https://crfm.stanford.edu/helm/) | Holistic Evaluation of Language Models | Stanford CRFM | Gemini Ultra (92.10%) |

### Benchmark Collections

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| Hugging Face Open LLM Leaderboard | [huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) | Community-driven open evaluation of general LLMs | Hugging Face | Claude 3.5 Sonnet, Gemini |
| Epoch AI Benchmarking Dashboard | [epochai.org/blog/benchmarking-dashboard](https://epochai.org/blog/benchmarking-dashboard) | Broad progress tracking across scientific and technical reasoning domains | Epoch AI | Claude 3.5, GPT-4, Gemini |
| LLM Leaderboard (Vellum AI) | [vellum.ai/llm-leaderboard](https://www.vellum.ai/llm-leaderboard) | Comparison across reasoning, math, and speed capabilities | Vellum AI | Claude 3.5, GPT-4, Gemini |
| LiveBench | [livebench.ai](https://livebench.ai/) | Reliable model evaluation with live performance updates | Academic/Independent | Varied |
| AlpacaEval | [tatsu-lab.github.io/alpaca_eval](https://tatsu-lab.github.io/alpaca_eval/) | Evaluating instruction-following capabilities | Stanford CRFM | GPT-4o |

### Community Evaluation

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| Reddit r/singularity | [reddit.com/r/singularity](https://www.reddit.com/r/singularity/) | Community leaderboards with crowd-sourced evaluations | Community Curated | Varied |

### Tool Selection & Usage

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| Galileo Agent Leaderboard | [huggingface.co/spaces/galileo-ai/agent-leaderboard](https://huggingface.co/spaces/galileo-ai/agent-leaderboard) | Evaluates AI models on Tool Selection Quality (TSQ) in real-world business scenarios | Galileo Team | Google gemini-2.0-flash-001 (TSQ 0.938) |

## Agent Evaluations

### Multi-Benchmark Assessment

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| HAL (Holistic Agent Leaderboard) | [hal.cs.princeton.edu](https://hal.cs.princeton.edu/) | Standardized, cost-aware evaluation across various benchmarks | Princeton | Various |

### General Capabilities

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| HAL GAIA Leaderboard | [hal.cs.princeton.edu/gaia](https://hal.cs.princeton.edu/gaia) | Evaluates agents on 450 questions requiring reasoning and tool-use | Princeton | H2O.ai h2oGPTe Agent (65%) |

### Safety Assessment

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| HAL AgentHarm Leaderboard | [hal.cs.princeton.edu/agentharm](https://hal.cs.princeton.edu/agentharm) | Evaluating AI agents on safety and harm prevention | Princeton | Not publicly reported |

### Cybersecurity Capabilities

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| HAL Cybench Leaderboard | [hal.cs.princeton.edu/cybench](https://hal.cs.princeton.edu/cybench) | Evaluating cybersecurity capabilities on CTF tasks | Princeton | Not publicly reported |

### Code Generation

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| HAL SWE-bench Verified Leaderboard | [hal.cs.princeton.edu/swebench](https://hal.cs.princeton.edu/swebench) | Evaluating code generation and bug fixing on real GitHub issues | Princeton | Claude 3.7 + O1 ensemble (65.4%) |

## AI Model Evaluations

### Frontier Capabilities

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| Scale AI SEAL Leaderboards | [scale.com/leaderboard](https://scale.com/leaderboard) | Neutral, tamper-proof leaderboards using private datasets | Scale | o1 (OpenAI) |
| Scale SEAL Agentic Tool Use Leaderboard | [scale.com/leaderboard/tool_use_enterprise](https://scale.com/leaderboard/tool_use_enterprise) | Evaluating frontier models on enterprise agentic tool use | Scale | o1 (December 2024) (70.14±5.32) |

### Complex Reasoning

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| Scale SEAL EnigmaEval Leaderboard | [scale.com/leaderboard/enigma_eval](https://scale.com/leaderboard/enigma_eval) | Uses puzzle hunts to test complex reasoning and problem-solving | Scale | o3 (medium) (April 2025) (13.09±1.92) |

## Intelligence and Reasoning

### Knowledge Breadth

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| MMLU | [github.com/hendrycks/test](https://github.com/hendrycks/test) | Massive Multitask Language Understanding | NYU, OpenAI | GPT-4o (95.30%) |

### General Intelligence

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| BIG-Bench | [github.com/google/BIG-bench](https://github.com/google/BIG-bench) | Beyond the Imitation Game Benchmark | Google, Community | GPT-4o (88.50%) |

## Mathematical Reasoning

### Elementary Math

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| GSM8K | [github.com/openai/grade-school-math](https://github.com/openai/grade-school-math) | Grade School Math 8K | Google Research | Claude 3.5 Opus (97.20%) |

## Coding and Software Engineering

### Code Generation

| Name | URL | Description | Maintainer | Top Models |
|------|-----|-------------|------------|------------|
| HumanEval | [github.com/openai/human-eval](https://github.com/openai/human-eval) | Evaluating code generation capabilities | OpenAI | Claude 3 Opus (89.80%) |

## Related Awesome Lists

- [Awesome General Agents Benchmark](https://github.com/supernalintelligence/Awesome-General-Agents-Benchmark-)
- [Awesome GUI Agents](https://github.com/supernalintelligence/Awesome-Gui-Agents-/blob/main/README.md)
- [Awesome General Agents Leaderboard](https://github.com/supernalintelligence/Awesome-General-Agents-Leaderboard)

## License

MIT

---

For more complete data and the latest information, please visit our website: [supernalintelligence.com](https://supernalintelligence.com)

If you see an error or want to contribute, please email [i@supernal.ai](mailto:i@supernal.ai)
